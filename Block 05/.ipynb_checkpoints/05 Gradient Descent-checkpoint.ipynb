{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "This notebook demonstrates the gradient descent approach to determine the best fitting parameters by linear regression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples Prof. Dr. Josef BÃ¼rgler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Toy Example\n",
    "Firstly, we demonstrate gradient descent on a simple linear regression problem with one dependent and one independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,1,2,3,4,5,6,7,8,9,10,10])\n",
    "y = np.array([1,2,3,1,4,5,6,4,7,10,15,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x and y values are plotted in a diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then try to fit the points by a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = -0.5\n",
    "theta1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta0, theta1):\n",
    "    y_pred = theta0 + theta1 * X\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X, theta0, theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_line(X, theta0, theta1, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(X.min()-1, X.max()+1, 1).reshape(-1,1)\n",
    "    y_pred = predict(x, theta0, theta1)\n",
    "    ax.plot(x, y_pred, color=\"r\")\n",
    "    \n",
    "ax = sns.scatterplot(X, y)\n",
    "plot_regression_line(X, theta0, theta1, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look so bad. Let's implement a gradient descent algorithm to do this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "We define a cost function that determines the mean squared error of the predicted and the actual y coordinates. To get rid of the factor 2 in the gradient\n",
    "formula, we divide the sum by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the MSE cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def cost(y, y_pred):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def cost(y, y_pred):\n",
    "    cost = np.sum((y_pred - y) ** 2) / (2 * len(y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate gradient\n",
    "\n",
    "Next, let us determine the gradient of y in respect to the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, theta0, theta1):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return grad_theta0, grad_theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, theta0, theta1):\n",
    "    y_pred = predict(X, theta0, theta1)\n",
    "    diff = y_pred - y\n",
    "    \n",
    "    n = len(X)\n",
    "    grad_theta0 = np.sum(diff) / n\n",
    "    grad_theta1 = np.dot(diff, X.T) / n\n",
    "    \n",
    "    return grad_theta0, grad_theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(X, y, theta0, theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now complete the `fit` function by iteratively updating our model parameters.\n",
    "\n",
    "To visualize how the parameters and cost functions change with each epoch, we store them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y, alpha, num_epochs, display_every=10):\n",
    "    theta0 = 0.0\n",
    "    theta1 = np.random.randn()\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # START YOUR CODE\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # END YOUR CODE\n",
    "        y_pred = predict(X, theta0, theta1)\n",
    "        curr_cost = cost(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        hist[\"theta0\"].append(theta0)\n",
    "        hist[\"theta1\"].append(theta1)\n",
    "\n",
    "        if epoch % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(epoch, curr_cost))\n",
    "        \n",
    "    return theta0, theta1, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def fit(X, y, alpha, num_epochs, display_every=10):\n",
    "    theta0 = 0.0\n",
    "    theta1 = np.random.randn()\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        grad_theta0, grad_theta1 = gradient(X, y, theta0, theta1)\n",
    "        theta0 = theta0 - alpha * grad_theta0\n",
    "        theta1 = theta1 - alpha * grad_theta1\n",
    "        \n",
    "        y_pred = predict(X, theta0, theta1)\n",
    "        curr_cost = cost(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        hist[\"theta0\"].append(theta0)\n",
    "        hist[\"theta1\"].append(theta1)\n",
    "\n",
    "        if epoch % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(epoch, curr_cost))\n",
    "        \n",
    "    return theta0, theta1, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "theta0, theta1, hist = fit(X, y, alpha, num_epochs, display_every=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learning \n",
    "We can now visualize the learning process by plotting the validation curve. The validation curve shows how the cost decreases by increasing number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(data, ax=None, ylim=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\"Validation Curve\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(data)\n",
    "    \n",
    "plot_validation_curve(hist[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our history, we can now visualize how the parameters change by each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(epoch=(1, len(hist[\"theta0\"])))\n",
    "def visualize_learning(epoch=1):\n",
    "    ax = sns.scatterplot(X, y)\n",
    "    plot_regression_line(X, hist[\"theta0\"][epoch-1], hist[\"theta1\"][epoch-1], ax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot\n",
    "We can visualize how our model parameters $\\Theta$ change after each epoch by displaying a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_cost(Theta0, Theta1, X, y):\n",
    "    m = Theta0.shape[0]\n",
    "    n = Theta0.shape[1]\n",
    "    tot = np.zeros((m,n))\n",
    "    for i in range(1,len(X)):\n",
    "        tot += (Theta0 + Theta1 * X[i] - y[i]) ** 2;\n",
    "    return tot/(2*len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['xtick.direction'] = 'out'\n",
    "matplotlib.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "def contour_plot(X, y, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "    delta = 0.025\n",
    "    t0 = np.arange(-0.5, 0.5, delta)\n",
    "    t1 = np.arange(0.5, 1.5, delta)\n",
    "    T0, T1 = np.meshgrid(t0, t1)\n",
    "    Z = parallel_cost(T0, T1, X, y)\n",
    "    CS = ax.contour(T0, T1, Z, levels = [0.25,0.5,1,2,3])\n",
    "    ax.clabel(CS, inline=1, fontsize=10)\n",
    "    ax.set_title('Contour plot')\n",
    "    ax.set_xlabel(r'$\\theta_0$')\n",
    "    ax.set_ylabel(r'$\\theta_1$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(epoch=(1, len(hist[\"theta0\"])))\n",
    "def visualize_contour_plot(epoch=1):\n",
    "    ax = contour_plot(X, y)\n",
    "    for i in range(epoch):\n",
    "        theta0 = hist[\"theta0\"][i]\n",
    "        theta1 = hist[\"theta1\"][i]\n",
    "        ax.plot(theta0, theta1, \"ro\", linewidth=9)\n",
    "        if i == 0: \n",
    "            continue\n",
    "            \n",
    "        theta0_prev = hist[\"theta0\"][i-1]\n",
    "        theta1_prev = hist[\"theta1\"][i-1]\n",
    "        \n",
    "        ax.annotate('', xy=[theta0, theta1], xytext=[theta0_prev, theta1_prev],\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise data\n",
    "Let's run the experiment above again but this time first normalise the data and see what happens.\n",
    "We use the `StandardScaler` which implements z-normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, 1)).reshape(-1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply gradient descent algorithm on normalised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "theta0, theta1, hist = fit(X, y, alpha, num_epochs, display_every=2)\n",
    "plot_validation_curve(hist[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like it did not converge yet. Let's increase the learning rate $\\alpha$ and the number of epochs and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "num_epochs = 50\n",
    "\n",
    "theta0, theta1, hist = fit(X, y, alpha, num_epochs, display_every=5)\n",
    "plot_validation_curve(hist[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better now. Okay, let's plot the contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(X, y, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "    delta = 0.025\n",
    "    t0 = np.arange(0, 9, delta)\n",
    "    t1 = np.arange(0, 9, delta)\n",
    "    T0, T1 = np.meshgrid(t0, t1)\n",
    "    Z = parallel_cost(T0, T1, X, y)\n",
    "    CS = ax.contour(T0, T1, Z, levels = [1,2,3,4,5,6])\n",
    "    ax.clabel(CS, inline=1, fontsize=10)\n",
    "    ax.set_title('Contour plot')\n",
    "    ax.set_xlabel(r'$\\theta_0$')\n",
    "    ax.set_ylabel(r'$\\theta_1$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact(epoch=(1, len(hist[\"theta0\"])))\n",
    "def visualize_contour_plot(epoch=1):\n",
    "    ax = contour_plot(X, y)\n",
    "    for i in range(epoch):\n",
    "        theta0 = hist[\"theta0\"][i]\n",
    "        theta1 = hist[\"theta1\"][i]\n",
    "        ax.plot(theta0, theta1, \"ro\", linewidth=9)\n",
    "        if i == 0: \n",
    "            continue\n",
    "            \n",
    "        theta0_prev = hist[\"theta0\"][i-1]\n",
    "        theta1_prev = hist[\"theta1\"][i-1]\n",
    "        \n",
    "        ax.annotate('', xy=[theta0, theta1], xytext=[theta0_prev, theta1_prev],\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contours are not as narrow as before. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "    Make sure that you never forget to scale your data before applying the gradient descent algorithm!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - House prices data set\n",
    "Now that we have tested our functions with our toy datset, let's move to a the house price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the price of a house based on its size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the feature from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Size\"]].values\n",
    "y = df.Price.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we further split the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(-1)\n",
    "X_test = X_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize our training data in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(X_train.reshape(-1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Batch Gradient Descent\n",
    "Let's use our implemented `fit` method to apply batch gradient descent to the house price dataset and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_epochs = 300\n",
    "\n",
    "theta0, theta1, hist = fit(X_train, y_train, alpha, num_epochs, display_every=20)\n",
    "plot_validation_curve(hist[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our gradient descent algorithm does not converge! \n",
    "\n",
    "> Why did that happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data\n",
    "Let's try it again but this time we will scale the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Size\"]].values\n",
    "y = df.Price.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> z-normalise the training and test data by using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# z-normalise the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).reshape(-1)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the gradient descent algorithm again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_epochs = 300\n",
    "\n",
    "theta0, theta1, hist = fit(X_train, y_train, alpha, num_epochs, display_every=20)\n",
    "plot_validation_curve(hist[\"cost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation curve looks much better now. We see that the cost converges after a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can visualize how our regression line looks after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(epoch=(1, len(hist[\"theta0\"])))\n",
    "def visualize_learning(epoch=1):\n",
    "    ax = sns.scatterplot(X_train, y_train)\n",
    "    plot_regression_line(X_train, hist[\"theta0\"][epoch-1], hist[\"theta1\"][epoch-1], ax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics on the test set\n",
    "> Now calculate the $R^2$ score on the test set by using the previously implemented `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, theta0, theta1)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 -  Autoscout data set\n",
    "We extend our code for multiple linear regression. We will use the autoscout dataset from the previous exercises. First we apply the data cleaning and then z-Normalise our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cars.csv')\n",
    "df.drop(['Name', 'Registration'], axis='columns', inplace=True)\n",
    "df.drop([17010, 7734, 47002, 44369, 24720, 50574, 36542, 42611,\n",
    "         22513, 12773, 21501, 2424, 52910, 29735, 43004, 47125], axis='rows', inplace=True)\n",
    "df.drop(df.index[df.EngineSize > 7500], axis='rows', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.head()\n",
    "\n",
    "numerical_cols = ['Price', 'Mileage', 'Horsepower', 'EngineSize']\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "q3 = train.loc[:, numerical_cols].describe().loc['75%']\n",
    "iqr = q3 - df.loc[:, numerical_cols].describe().loc['25%']\n",
    "upper_boundary = q3 + 1.5*iqr\n",
    "upper_boundary\n",
    "\n",
    "# And here the outliers are removed\n",
    "train = train[(train.Price <= upper_boundary.Price) &\n",
    "        (train.Mileage <= upper_boundary.Mileage) &\n",
    "        (train.Horsepower <= upper_boundary.Horsepower) &\n",
    "        (train.EngineSize <= upper_boundary.EngineSize)]\n",
    "\n",
    "test = test[(test.Price <= upper_boundary.Price) &\n",
    "        (test.Mileage <= upper_boundary.Mileage) &\n",
    "        (test.Horsepower <= upper_boundary.Horsepower) &\n",
    "        (test.EngineSize <= upper_boundary.EngineSize)]\n",
    "\n",
    "X_train = train.drop(columns=[\"Price\"]).values\n",
    "X_test = test.drop(columns=[\"Price\"]).values\n",
    "\n",
    "y_train = train.Price.values\n",
    "y_test = test.Price.values\n",
    "\n",
    "# z-Normalise the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify our predict function that instead of providing $\\theta_0$ and $\\theta_1$ we now provide the bias ($\\theta_0$) and the other parameters $\\Theta$ as an array. \n",
    "> Implement the `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(X, bias, thetas):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def predict(X, bias, thetas):\n",
    "    y_pred = bias + np.dot(X, thetas)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, bias, thetas):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, bias, thetas):\n",
    "    y_pred = predict(X, bias, thetas)\n",
    "    diff = y_pred - y\n",
    "    \n",
    "    n = len(X)\n",
    "    grad_bias = np.sum(diff) / n\n",
    "    grad_thetas = np.dot(diff, X) / n\n",
    "    \n",
    "    return grad_bias, grad_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend our `fit` function by tracking not only the cost but also the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, alpha, num_epochs, display_every=50):\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, X_train.shape[1])).reshape(-1)\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for epoch in tqdm(range(1, num_epochs+1)):\n",
    "        grad_bias, grad_thetas = gradient(X_train, y_train, bias, thetas)\n",
    "        bias = bias - alpha * grad_bias\n",
    "        thetas = thetas - alpha * grad_thetas\n",
    "        \n",
    "        y_pred_train = predict(X_train, bias, thetas)\n",
    "        train_cost = cost(y_train, y_pred_train)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        \n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0:\n",
    "            print(\"Epoch {0} - cost: {1:.2} - r2: {2:.4}\"\n",
    "                  .format(epoch, train_cost, train_r2))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_epochs = 1000\n",
    "bias, thetas, hist = fit(X_train, y_train, alpha, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curves(hist, ylim=None):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "\n",
    "    ax[0].set_title(\"Train Cost\")\n",
    "    ax[0].set_ylabel(\"Cost\")\n",
    "    plot_validation_curve(hist[\"train_cost\"], ax[0], ylim)\n",
    "\n",
    "    ax[1].set_title(\"Train R2\")\n",
    "    ax[1].set_ylabel(\"R2\")\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    plot_validation_curve(hist[\"train_r2\"], ax[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_validation_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics on test set\n",
    "Now we calculate the $R^2$ score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, bias, thetas)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous exercise where we calculated the estimates for the $\\Theta$ numerically using the normal equation we got almost the same result with the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now  modify our `fit` function to use mini batch gradient descent. So instead of calculating the gradient on the whole dataset on each step, only use a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, alpha, num_epochs, batch_size, display_every=50):\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, X_train.shape[1])).reshape(-1)\n",
    "    hist = defaultdict(list)\n",
    "    \n",
    "    indices_train = np.arange(len(X_train))\n",
    "    \n",
    "    num_samples = len(X_train)\n",
    "    steps = int(num_samples/batch_size)\n",
    "    \n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # randomize inputs\n",
    "        np.random.shuffle(indices_train)\n",
    "        \n",
    "        X_train_epoch = X_train[indices_train]\n",
    "        y_train_epoch = y_train[indices_train]\n",
    "        \n",
    "        for step in range(steps):\n",
    "            start = step * batch_size\n",
    "            end = step * batch_size + batch_size\n",
    "            \n",
    "            X_train_mini = X_train_epoch[start:end]\n",
    "            y_train_mini = y_train_epoch[start:end]\n",
    "        \n",
    "            # START YOUR CODE\n",
    "            # Apply gradient descent\n",
    "            \n",
    "            \n",
    "            \n",
    "            # END YOUR CODE\n",
    "\n",
    "        y_pred_train = predict(X_train, bias, thetas)\n",
    "        \n",
    "        train_cost = cost(y_train, y_pred_train)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0 or epoch == num_epochs:\n",
    "            print(\"Epoch {0} - train_cost: {1:.2} - train_r2: {2:.4}\".format(epoch, train_cost, train_r2))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, alpha, num_epochs, batch_size, display_every=50):\n",
    "    bias = 0.0\n",
    "    thetas = np.random.randn(*(1, X_train.shape[1])).reshape(-1)\n",
    "    hist = defaultdict(list)\n",
    "    \n",
    "    indices_train = np.arange(len(X_train))   \n",
    "    \n",
    "    num_samples = len(X_train)\n",
    "    steps = int(num_samples/batch_size)\n",
    "    \n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # randomize inputs\n",
    "        np.random.shuffle(indices_train)\n",
    "        \n",
    "        X_train_epoch = X_train[indices_train]\n",
    "        y_train_epoch = y_train[indices_train]\n",
    "        \n",
    "        for step in range(steps):\n",
    "            start = step * batch_size\n",
    "            end = step * batch_size + batch_size\n",
    "            \n",
    "            X_train_mini = X_train_epoch[start:end]\n",
    "            y_train_mini = y_train_epoch[start:end]\n",
    "        \n",
    "            grad_bias, grad_thetas = gradient(X_train_mini, y_train_mini, bias, thetas)\n",
    "            bias = bias - alpha * grad_bias\n",
    "            thetas = thetas - alpha * grad_thetas\n",
    "\n",
    "        y_pred_train = predict(X_train, bias, thetas)\n",
    "        \n",
    "        train_cost = cost(y_train, y_pred_train)\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "        hist[\"train_cost\"].append(train_cost)\n",
    "        hist[\"train_r2\"].append(train_r2)\n",
    "        \n",
    "        if epoch % display_every == 0 or epoch == num_epochs:\n",
    "            print(\"Epoch {0} - train_cost: {1:.2} - train_r2: {2:.4}\".format(epoch, train_cost, train_r2))\n",
    "        \n",
    "    return bias, thetas, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo have now introduced an additional hyperparameter `batch_size`. \n",
    "* If we set `batch_size` equal to 1, we use Stochastic Gradient Desccent: We update our model parameters $\\Theta$ for each training example. \n",
    "* If we set `batch_size` equal to to the number of training samples we have again Batch Gradient Descent: We use all training samples to update the model parameters $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent\n",
    "We run batch gradient descent and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-2\n",
    "num_epochs = 50\n",
    "batch_size = len(X_train)\n",
    "\n",
    "bias, thetas, hist = fit(X_train, y_train, alpha, num_epochs, batch_size)\n",
    "plot_validation_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the following:\n",
    "* The training did not converge after those 50 epochs. We would need more epochs.\n",
    "* The training cost is strictly decreasing as we take all training samples per step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minibatch Gradient Descnet\n",
    "Let's compare it to minibatch gradient descent with a `batch_size` of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-2\n",
    "num_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "bias, thetas, hist = fit(X_train, y_train, alpha, num_epochs, batch_size)\n",
    "plot_validation_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we are taking only a subset of our data when applying gradient descent, the training cost are not stricly decreasing anymore. \n",
    "* We do not need as many epochs as before as we are doing much more updates now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, bias, thetas)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the ILIAS Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that you have implemented the gradient descent algorithm from scratch, you're ready to answer the ILIAS Quiz **Gradient Descent**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
